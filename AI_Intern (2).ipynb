{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRpI3o1cHv5Q"
      },
      "outputs": [],
      "source": [
        "#@title 1. Installing Required Libraries\n",
        "# Install necessary libraries for web scraping, language model interaction, and vector search.\n",
        "\n",
        "!pip install googletrans==4.0.0rc1 # Install specific compatible version\n",
        "!pip install requests beautifulsoup4 pandas\n",
        "!pip install accelerate # Add accelerate for faster loading\n",
        "!pip install transformers torch\n",
        "!pip install -U sentence-transformers\n",
        "!pip install faiss-cpu\n",
        "\n",
        "\n",
        "\n",
        "#@title 2. Scrape FAQs from Jupiter Money Website\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def scrape_jupiter_faqs():\n",
        "    \"\"\"\n",
        "    Scrapes the FAQ section from the Jupiter Money contact page.\n",
        "    Returns:\n",
        "        list: A list of dictionaries, where each dictionary represents a category\n",
        "              and contains the category title and a list of Q&A pairs.\n",
        "              Returns an empty list if scraping fails.\n",
        "    \"\"\"\n",
        "    print(\"Starting FAQ scraping...\")\n",
        "    URL = \"https://jupiter.money/contact/\"\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(URL, headers=headers)\n",
        "        response.raise_for_status()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching the URL: {e}\")\n",
        "        return []\n",
        "\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    faq_data = []\n",
        "    faq_list = soup.find('ul', attrs={'data-controller': 'faq'})\n",
        "\n",
        "    if not faq_list:\n",
        "        print(\"No FAQ list found with 'data-controller=faq'. The website structure might have changed.\")\n",
        "        return []\n",
        "\n",
        "    category_title = \"Frequently Asked Questions\"\n",
        "    qna_pairs = []\n",
        "\n",
        "    faq_items = faq_list.find_all('li')\n",
        "\n",
        "    print(f\"Found {len(faq_items)} FAQ items.\")\n",
        "\n",
        "    for item in faq_items:\n",
        "        question_tag = item.find('button').find('span')\n",
        "        answer_tag = item.find('p')\n",
        "\n",
        "        if question_tag and answer_tag:\n",
        "            question = question_tag.get_text(strip=True)\n",
        "            answer = answer_tag.get_text(strip=True)\n",
        "\n",
        "            question = re.sub(r'\\s+', ' ', question).strip()\n",
        "            answer = re.sub(r'\\s+', ' ', answer).strip()\n",
        "\n",
        "            if question and answer:\n",
        "                qna_pairs.append({\"question\": question, \"answer\": answer})\n",
        "\n",
        "    if qna_pairs:\n",
        "        faq_data.append({\n",
        "            \"category\": category_title,\n",
        "            \"qna\": qna_pairs\n",
        "        })\n",
        "        print(f\"  - Scraped {len(qna_pairs)} Q&As from '{category_title}'\")\n",
        "\n",
        "    print(\"Scraping finished successfully.\")\n",
        "    return faq_data\n",
        "\n",
        "# Execute the scraping function\n",
        "scraped_data = scrape_jupiter_faqs()\n",
        "\n",
        "# Convert the structured data into a flat DataFrame for easier processing\n",
        "if scraped_data:\n",
        "    flat_data = []\n",
        "    for category_info in scraped_data:\n",
        "        for qna in category_info['qna']:\n",
        "            flat_data.append({\n",
        "                \"category\": category_info['category'],\n",
        "                \"question\": qna['question'],\n",
        "                \"answer\": qna['answer']\n",
        "            })\n",
        "\n",
        "    df_faqs = pd.DataFrame(flat_data)\n",
        "    print(\"\\nFAQ data converted to DataFrame (first 5 rows):\")\n",
        "    print(df_faqs.head())\n",
        "    print(f\"\\nTotal FAQs scraped: {len(df_faqs)}\")\n",
        "else:\n",
        "    print(\"\\nCould not scrape any data. Please check the URL and website structure.\")\n",
        "    df_faqs = pd.DataFrame() # Create an empty dataframe to avoid errors later\n",
        "\n",
        "#@title 3. Preprocess and Clean the Data\n",
        "def preprocess_and_clean(df):\n",
        "    \"\"\"\n",
        "    Cleans and preprocesses the FAQ DataFrame.\n",
        "    \"\"\"\n",
        "    if df.empty:\n",
        "        print(\"DataFrame is empty. Skipping preprocessing.\")\n",
        "        return df\n",
        "\n",
        "    print(\"\\nStarting preprocessing and cleaning...\")\n",
        "    df['question'] = df['question'].apply(lambda x: BeautifulSoup(x, \"html.parser\").get_text())\n",
        "    df['answer'] = df['answer'].apply(lambda x: BeautifulSoup(x, \"html.parser\").get_text())\n",
        "\n",
        "    df['question'] = df['question'].apply(lambda x: re.sub(r'\\s+', ' ', x).strip())\n",
        "    df['answer'] = df['answer'].apply(lambda x: re.sub(r'\\s+', ' ', x).strip())\n",
        "\n",
        "    initial_rows = len(df)\n",
        "    df.drop_duplicates(subset=['question'], inplace=True, keep='first')\n",
        "    final_rows = len(df)\n",
        "\n",
        "    print(f\"Removed {initial_rows - final_rows} duplicate questions.\")\n",
        "    print(\"Preprocessing finished.\")\n",
        "    return df\n",
        "\n",
        "if not df_faqs.empty:\n",
        "    df_cleaned = preprocess_and_clean(df_faqs.copy())\n",
        "    print(\"\\nCleaned DataFrame sample:\")\n",
        "    print(df_cleaned.head())\n",
        "    df_cleaned.to_csv(\"jupiter_faqs_cleaned.csv\", index=False)\n",
        "    print(\"\\nCleaned data saved to 'jupiter_faqs_cleaned.csv'\")\n",
        "else:\n",
        "    df_cleaned = pd.DataFrame()\n",
        "\n",
        "\n",
        "#@title 4. Build Semantic Search Index with FAISS\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def build_faiss_index(df):\n",
        "    \"\"\"\n",
        "    Builds a FAISS index for semantic search using question embeddings.\n",
        "    \"\"\"\n",
        "    if df.empty or 'question' not in df.columns:\n",
        "        print(\"DataFrame is empty or missing 'question' column. Cannot build index.\")\n",
        "        return None, None\n",
        "\n",
        "    print(\"\\nBuilding FAISS index for semantic search...\")\n",
        "    model_name = 'all-MiniLM-L6-v2'\n",
        "\n",
        "    model_path = os.path.join(os.path.expanduser('~/.cache/torch/sentence_transformers'), model_name.replace('/', '_'))\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"Downloading SentenceTransformer model: {model_name}...\")\n",
        "\n",
        "    model = SentenceTransformer(model_name)\n",
        "\n",
        "    print(\"Generating embeddings for all questions...\")\n",
        "    questions = df['question'].tolist()\n",
        "    embeddings = model.encode(questions, convert_to_tensor=True, show_progress_bar=True)\n",
        "\n",
        "    embeddings_np = embeddings.cpu().numpy().astype('float32')\n",
        "\n",
        "    d = embeddings_np.shape[1]\n",
        "    index = faiss.IndexFlatL2(d)\n",
        "\n",
        "    index.add(embeddings_np)\n",
        "\n",
        "    print(f\"FAISS index built successfully with {index.ntotal} vectors.\")\n",
        "    return index, model\n",
        "\n",
        "if not df_cleaned.empty:\n",
        "    faiss_index, embedding_model = build_faiss_index(df_cleaned)\n",
        "else:\n",
        "    faiss_index, embedding_model = None, None\n",
        "\n",
        "def search_faqs(query_in_english, index, model, df, top_k=3):\n",
        "    \"\"\"\n",
        "    Searches for relevant FAQs using the FAISS index.\n",
        "    \"\"\"\n",
        "    if index is None or model is None:\n",
        "        print(\"FAISS index or embedding model not initialized. Cannot perform search.\")\n",
        "        return []\n",
        "\n",
        "    query_embedding = model.encode([query_in_english], convert_to_tensor=True).cpu().numpy().astype('float32')\n",
        "\n",
        "    distances, indices = index.search(query_embedding, top_k)\n",
        "\n",
        "    results = []\n",
        "    for i in indices[0]:\n",
        "        results.append({\n",
        "            \"question\": df.iloc[i]['question'],\n",
        "            \"answer\": df.iloc[i]['answer'],\n",
        "            \"distance\": distances[0][np.where(indices[0] == i)[0][0]]\n",
        "        })\n",
        "    return results\n",
        "\n",
        "#@title 5. Setup the LLM (microsoft/Phi-4-mini-instruct) for Response Generation\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from googletrans import Translator\n",
        "\n",
        "# Initialize Google Translator globally\n",
        "google_translator = Translator()\n",
        "\n",
        "def setup_llm_pipeline():\n",
        "    \"\"\"\n",
        "    Sets up the LLM pipeline for generating answers.\n",
        "    Uses 'microsoft/Phi-4-mini-instruct'.\n",
        "    \"\"\"\n",
        "    print(\"Setting up the LLM...\")\n",
        "    model_name = \"microsoft/Phi-4-mini-instruct\"\n",
        "\n",
        "    try:\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            device_map=\"auto\",\n",
        "            torch_dtype=\"auto\",\n",
        "            trust_remote_code=True,\n",
        "        )\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model '{model_name}': {e}\")\n",
        "        print(\"Please ensure the model name is correct, you have sufficient resources (RAM/GPU),\")\n",
        "        print(\"and you have accepted its license on the Hugging Face Hub if it's a gated model.\")\n",
        "        return None\n",
        "\n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "    print(\"LLM pipeline ready.\")\n",
        "    return pipe\n",
        "\n",
        "# Initialize the LLM\n",
        "llm_pipeline = setup_llm_pipeline()\n",
        "\n",
        "#@title 6. Build the RAG-based FAQ Bot with Suggestion Capabilities\n",
        "import time # Keep for internal profiling, remove if truly not needed anywhere\n",
        "import math\n",
        "\n",
        "class FAQBot:\n",
        "    \"\"\"\n",
        "    A class that encapsulates the entire FAQ Bot functionality, with googletrans handling\n",
        "    auto-detection, translation to English for search, and translation back to user's language.\n",
        "    Includes functionality for suggesting related queries based on user history and directly from FAQs.\n",
        "    \"\"\"\n",
        "    def __init__(self, df, index, model, pipe):\n",
        "        self.df = df\n",
        "        self.index = index\n",
        "        self.embedding_model = model\n",
        "        self.llm_pipe = pipe\n",
        "\n",
        "        if self.df.empty or self.index is None or self.embedding_model is None:\n",
        "            raise ValueError(\"Bot components are not initialized. Please run previous cells successfully.\")\n",
        "        if self.llm_pipe is None:\n",
        "            raise ValueError(\"LLM pipeline not initialized. Cannot run bot without it.\")\n",
        "\n",
        "        self.query_history = [] # Stores {'query': str (original), 'embedding': np.array (English), 'language': str (detected), 'english_search_query': str}\n",
        "        self.pending_direct_faq_suggestion = None # Stores (original_user_query, matched_faq_q_a, detected_language)\n",
        "        self.direct_faq_threshold = 0.85 # Cosine similarity threshold for direct FAQ suggestion\n",
        "\n",
        "    def _auto_detect_and_translate_to_english(self, query):\n",
        "        \"\"\"\n",
        "        Uses googletrans to detect the language of the query and translate it to English.\n",
        "        Returns a tuple: (detected_language_code, translated_english_query)\n",
        "        \"\"\"\n",
        "        try:\n",
        "            detected = google_translator.detect(query)\n",
        "            detected_lang_code = detected.lang\n",
        "            # print(f\"Detected language: {detected_lang_code}\") # Internal debug print\n",
        "\n",
        "            if detected_lang_code != 'en':\n",
        "                # print(f\"Translating query from {detected_lang_code} to English...\") # Internal debug print\n",
        "                translated_obj = google_translator.translate(query, dest='en')\n",
        "                translated_text = translated_obj.text\n",
        "                # print(f\"Translated query to English: '{translated_text}'\") # Internal debug print\n",
        "                return detected_lang_code, translated_text\n",
        "            else:\n",
        "                # print(\"Query is already in English.\") # Internal debug print\n",
        "                return 'en', query\n",
        "        except Exception as e:\n",
        "            print(f\"Error during language detection or translation to English: {e}\")\n",
        "            return 'en', query # Fallback\n",
        "\n",
        "    def _translate_english_response_back_and_combine(self, english_response, target_language_code):\n",
        "        \"\"\"\n",
        "        Translates an English response back to the target language if not English,\n",
        "        and combines both English and the translated version into a single string.\n",
        "        \"\"\"\n",
        "        if target_language_code == 'en':\n",
        "            # print(\"Target language is English, providing only English response.\") # Internal debug print\n",
        "            return english_response\n",
        "\n",
        "        # print(f\"Translating English response to {target_language_code} and combining...\") # Internal debug print\n",
        "        try:\n",
        "            translated_obj = google_translator.translate(english_response, dest=target_language_code)\n",
        "            translated_text = translated_obj.text\n",
        "\n",
        "            combined_response = (\n",
        "                f\"English: {english_response}\\n\\n\"\n",
        "                f\"Your language ({target_language_code}): {translated_text}\"\n",
        "            )\n",
        "            # print(\"Combined response generated.\") # Internal debug print\n",
        "            return combined_response\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating response back to {target_language_code}: {e}\")\n",
        "            return f\"Sorry, I am having trouble translating my answer to your language right now, but here is the English response:\\n\\n{english_response}\"\n",
        "\n",
        "    def _generate_english_llm_response(self, original_query_for_context, context, llm_pipe):\n",
        "        \"\"\"\n",
        "        Generates a conversational answer in English using the LLM with RAG.\n",
        "        This function explicitly instructs the LLM to generate in English.\n",
        "        \"\"\"\n",
        "        if not llm_pipe:\n",
        "            return \"Sorry, my brain isn't working right now. Please try again later.\"\n",
        "        if not context:\n",
        "            return \"I couldn't find any specific information about that in the Jupiter Help Centre. Can you try asking in a different way?\"\n",
        "\n",
        "        # print(\"\\nGenerating final response with LLM in English...\") # Internal debug print\n",
        "\n",
        "        context_str = \"\\n\\n\".join([f\"Question: {c['question']}\\nAnswer: {c['answer']}\" for c in context])\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "You are a friendly and helpful customer support assistant for Jupiter, a digital banking app.\n",
        "Your job is to answer the user's question in a simple, conversational way, based ONLY on the provided context from the Jupiter Help Centre.\n",
        "Do not make up information. If the context does not contain the answer, clearly state that you couldn't find the information.\n",
        "It is crucial that your response is in **English**.\n",
        "\n",
        "Here is the relevant information I found from the Jupiter Help Centre:\n",
        "--- CONTEXT ---\n",
        "{context_str}\n",
        "--- END CONTEXT ---\n",
        "\n",
        "Now, please answer the following user's question.\n",
        "\n",
        "User's Question: \"{original_query_for_context}\"\n",
        "\n",
        "Your Answer (in conversational and friendly English):\n",
        "\"\"\"\n",
        "\n",
        "        generation_args = {\n",
        "            \"max_new_tokens\": 250,\n",
        "            \"return_full_text\": False,\n",
        "            \"temperature\": 0.5,\n",
        "            \"do_sample\": True,\n",
        "            \"top_p\": 0.9,\n",
        "            \"eos_token_id\": llm_pipe.tokenizer.eos_token_id,\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            output = llm_pipe(prompt, **generation_args)\n",
        "            response = output[0]['generated_text'].strip()\n",
        "\n",
        "            response = re.sub(r'Your Answer \\(in conversational and friendly English\\):', '', response, flags=re.IGNORECASE).strip()\n",
        "            response = re.sub(r'English:', '', response, flags=re.IGNORECASE).strip()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during LLM generation: {e}\")\n",
        "            response = \"I am having trouble generating a response right now. Please try again after some time.\"\n",
        "\n",
        "        # print(\"English LLM response generated.\") # Internal debug print\n",
        "        return response\n",
        "\n",
        "    def _search_faq_for_direct_suggestion(self, query_in_english):\n",
        "        \"\"\"\n",
        "        Searches the FAQ questions directly for a very close semantic match.\n",
        "        If a strong match is found, it returns the matched FAQ question and its answer.\n",
        "        \"\"\"\n",
        "        if self.index is None or self.embedding_model is None:\n",
        "            print(\"FAISS index or embedding model not initialized. Cannot perform direct FAQ search.\")\n",
        "            return None, None, 0.0\n",
        "\n",
        "        # print(f\"Searching FAQs for a direct suggestion (threshold: {self.direct_faq_threshold})...\") # Internal debug print\n",
        "        query_embedding = self.embedding_model.encode([query_in_english], convert_to_tensor=True).cpu().numpy().astype('float32')\n",
        "\n",
        "        distances, indices = self.index.search(query_embedding, 1)\n",
        "\n",
        "        if indices[0][0] != -1:\n",
        "            matched_idx = indices[0][0]\n",
        "            matched_question = self.df.iloc[matched_idx]['question']\n",
        "            matched_answer = self.df.iloc[matched_idx]['answer']\n",
        "\n",
        "            matched_embedding = self.embedding_model.encode([matched_question], convert_to_tensor=True).cpu().numpy().astype('float32')\n",
        "\n",
        "            # Compute cosine similarity\n",
        "            # Handle zero-norm case\n",
        "            norm_query = np.linalg.norm(query_embedding)\n",
        "            norm_matched = np.linalg.norm(matched_embedding)\n",
        "\n",
        "            if norm_query == 0 or norm_matched == 0:\n",
        "                cosine_sim = 0.0 # Or handle as an error/no similarity\n",
        "            else:\n",
        "                cosine_sim = np.dot(query_embedding.flatten(), matched_embedding.flatten()) / (norm_query * norm_matched)\n",
        "\n",
        "            # print(f\"Top FAQ match: '{matched_question}' with similarity: {cosine_sim:.3f}\") # Internal debug print\n",
        "\n",
        "            if cosine_sim >= self.direct_faq_threshold:\n",
        "                return {\"question\": matched_question, \"answer\": matched_answer}, cosine_sim\n",
        "        return None, 0.0\n",
        "\n",
        "    def suggest_related_queries(self, current_query_english_embedding, num_suggestions=3, min_similarity=0.75):\n",
        "        \"\"\"\n",
        "        Suggests related queries based on historical queries that are semantically similar\n",
        "        to the current query. Excludes the current query itself and deduplicates suggestions.\n",
        "        \"\"\"\n",
        "        if len(self.query_history) < 2:\n",
        "            return []\n",
        "\n",
        "        history_embeddings = []\n",
        "        history_original_queries = []\n",
        "\n",
        "        # Exclude the very last query (the current one being processed) from history for suggestions\n",
        "        for past_query_data in self.query_history[:-1]:\n",
        "            history_embeddings.append(past_query_data['embedding'])\n",
        "            history_original_queries.append(past_query_data['query'])\n",
        "\n",
        "        if not history_embeddings:\n",
        "            return []\n",
        "\n",
        "        history_embeddings_np = np.array(history_embeddings).astype('float32')\n",
        "\n",
        "        current_query_english_embedding_2d = current_query_english_embedding.reshape(1, -1)\n",
        "\n",
        "        d = history_embeddings_np.shape[1]\n",
        "        temp_index = faiss.IndexFlatL2(d)\n",
        "        temp_index.add(history_embeddings_np)\n",
        "\n",
        "        # Search for similar queries in history. Get enough candidates to filter.\n",
        "        distances, indices = temp_index.search(current_query_english_embedding_2d, num_suggestions * 5)\n",
        "\n",
        "        suggested_queries_set = set()\n",
        "        for idx in indices[0]:\n",
        "            if idx == -1: continue\n",
        "\n",
        "            # Recalculate cosine similarity for precision\n",
        "            # Handle zero-norm case for embeddings\n",
        "            norm_current = np.linalg.norm(current_query_english_embedding)\n",
        "            norm_past = np.linalg.norm(history_embeddings_np[idx])\n",
        "\n",
        "            if norm_current == 0 or norm_past == 0:\n",
        "                cosine_sim = 0.0\n",
        "            else:\n",
        "                cosine_sim = np.dot(current_query_english_embedding, history_embeddings_np[idx]) / (norm_current * norm_past)\n",
        "\n",
        "\n",
        "            if cosine_sim >= min_similarity:\n",
        "                suggested_query_original_text = history_original_queries[idx]\n",
        "                # Avoid suggesting the exact query the user just asked (based on its original form)\n",
        "                if suggested_query_original_text.lower().strip() != self.query_history[-1]['query'].lower().strip():\n",
        "                    suggested_queries_set.add(suggested_query_original_text)\n",
        "                if len(suggested_queries_set) >= num_suggestions:\n",
        "                    break\n",
        "\n",
        "        return list(suggested_queries_set)\n",
        "\n",
        "\n",
        "    def ask(self, user_query):\n",
        "        \"\"\"\n",
        "        Handles a user query from end to end, with auto-detection, translation for search,\n",
        "        and provides direct FAQ answers or RAG-generated answers.\n",
        "        Manages the direct FAQ suggestion flow.\n",
        "        Returns:\n",
        "            str: The final bot response.\n",
        "        \"\"\"\n",
        "        original_user_query = user_query\n",
        "        start_time_e2e = time.time() # For internal profiling\n",
        "\n",
        "        # --- Handle pending direct FAQ suggestion ---\n",
        "        if self.pending_direct_faq_suggestion:\n",
        "            prev_original_query = self.pending_direct_faq_suggestion['original_query']\n",
        "            matched_faq_q_a = self.pending_direct_faq_suggestion['suggested_faq_q_a']\n",
        "            prev_detected_language = self.pending_direct_faq_suggestion['detected_language']\n",
        "\n",
        "            # Translate 'yes'/'no' to the detected language from the previous turn for robust check\n",
        "            try:\n",
        "                translated_yes = google_translator.translate(\"yes\", dest=prev_detected_language).text.lower().strip()\n",
        "                translated_no = google_translator.translate(\"no\", dest=prev_detected_language).text.lower().strip()\n",
        "            except Exception as e:\n",
        "                # Fallback if translation for 'yes'/'no' fails\n",
        "                translated_yes = \"yes\"\n",
        "                translated_no = \"no\"\n",
        "                print(f\"Warning: Could not translate 'yes'/'no' for comparison due to {e}. Using English.\")\n",
        "\n",
        "\n",
        "            self.pending_direct_faq_suggestion = None # Clear the pending state after processing user's reply to suggestion\n",
        "\n",
        "            if user_query.lower().strip() == 'yes' or user_query.lower().strip() == translated_yes:\n",
        "                # User accepted the suggestion\n",
        "                final_answer = self._translate_english_response_back_and_combine(matched_faq_q_a['answer'], prev_detected_language)\n",
        "                end_time_e2e = time.time()\n",
        "                # print(f\"Internal: Handled direct FAQ acceptance. Latency: {end_time_e2e - start_time_e2e:.2f}s\") # Internal debug print\n",
        "                return final_answer\n",
        "            else:\n",
        "                # User rejected the suggestion or typed something else. Proceed with RAG for the *previous* query.\n",
        "                # print(\"Internal: User rejected direct FAQ suggestion. Proceeding with RAG for previous query.\") # Internal debug print\n",
        "                detected_language_for_rag, query_for_rag_search = self._auto_detect_and_translate_to_english(prev_original_query)\n",
        "                # Fall through to the RAG section below, using prev_original_query and detected_language_for_rag\n",
        "        else:\n",
        "            # Not in a pending state, process the new query\n",
        "            detected_language_for_rag, query_for_rag_search = self._auto_detect_and_translate_to_english(user_query)\n",
        "\n",
        "\n",
        "        query_embedding_np = self.embedding_model.encode([query_for_rag_search], convert_to_tensor=True).cpu().numpy().astype('float32')[0]\n",
        "\n",
        "        # Log the query and its English embedding for suggestion capabilities\n",
        "        self.query_history.append({\n",
        "            'query': original_user_query, # Store original query\n",
        "            'embedding': query_embedding_np,\n",
        "            'language': detected_language_for_rag,\n",
        "            'english_search_query': query_for_rag_search\n",
        "        })\n",
        "\n",
        "        # --- Try to find a direct FAQ match for suggestion for the *current* query (if not handling a rejected suggestion) ---\n",
        "        if self.pending_direct_faq_suggestion is None: # Only suggest if not already in a suggestion flow\n",
        "            matched_faq_q_a, similarity = self._search_faq_for_direct_suggestion(query_for_rag_search)\n",
        "            if matched_faq_q_a and similarity >= self.direct_faq_threshold:\n",
        "                # Set the pending state for the next user input\n",
        "                self.pending_direct_faq_suggestion = {\n",
        "                    'original_query': original_user_query,\n",
        "                    'suggested_faq_q_a': matched_faq_q_a,\n",
        "                    'detected_language': detected_language_for_rag\n",
        "                }\n",
        "\n",
        "                # Formulate the \"Did you mean?\" suggestion message in the user's language\n",
        "                try:\n",
        "                    suggested_q_translated = google_translator.translate(matched_faq_q_a['question'], dest=detected_language_for_rag).text\n",
        "                    yes_option = google_translator.translate(\"yes\", dest=detected_language_for_rag).text\n",
        "                    no_option = google_translator.translate(\"no\", dest=detected_language_for_rag).text\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning: Could not translate suggestion to {detected_language_for_rag} due to {e}. Using English for suggestion.\")\n",
        "                    suggested_q_translated = matched_faq_q_a['question']\n",
        "                    yes_option = \"yes\"\n",
        "                    no_option = \"no\"\n",
        "\n",
        "                suggestion_message = (\n",
        "                    f\"Did you mean: **'{suggested_q_translated}'**?\\n\"\n",
        "                    f\"(English: '{matched_faq_q_a['question']}')\\n\\n\"\n",
        "                    f\"Please type '{yes_option}' for Yes, or '{no_option}' for No.\"\n",
        "                )\n",
        "                end_time_e2e = time.time()\n",
        "                # print(f\"Internal: Direct FAQ suggestion issued. Latency: {end_time_e2e - start_time_e2e:.2f}s\") # Internal debug print\n",
        "                return suggestion_message\n",
        "\n",
        "        # --- If no direct FAQ suggestion, or if handling a rejected suggestion, proceed with RAG ---\n",
        "        retrieved_context = search_faqs(query_for_rag_search, self.index, self.embedding_model, self.df, top_k=3)\n",
        "\n",
        "        english_llm_response = self._generate_english_llm_response(original_user_query, retrieved_context, self.llm_pipe)\n",
        "        final_answer_str = self._translate_english_response_back_and_combine(english_llm_response, detected_language_for_rag)\n",
        "\n",
        "        # --- Add historical suggestions (if any) ---\n",
        "        suggestions = self.suggest_related_queries(query_embedding_np)\n",
        "        if suggestions:\n",
        "            suggestion_str = \"\\n\\nðŸ’¡ Perhaps you also want to know:\\n\"\n",
        "            for i, s in enumerate(suggestions):\n",
        "                suggestion_str += f\"- {i+1}. {s}\\n\"\n",
        "            final_answer_str += suggestion_str\n",
        "\n",
        "        end_time_e2e = time.time()\n",
        "        # print(f\"Internal: RAG answer generated. Latency: {end_time_e2e - start_time_e2e:.2f}s\") # Internal debug print\n",
        "        return final_answer_str\n",
        "\n",
        "\n",
        "# Initialize the bot if all components are ready\n",
        "jupiter_bot = None\n",
        "if not df_cleaned.empty and faiss_index is not None and embedding_model is not None and llm_pipeline is not None:\n",
        "    try:\n",
        "        jupiter_bot = FAQBot(df_cleaned, faiss_index, embedding_model, llm_pipeline)\n",
        "        print(\"\\nJupiter FAQ Bot is ready to answer your questions!\")\n",
        "    except ValueError as e:\n",
        "        print(f\"\\nBot initialization failed: {e}\")\n",
        "else:\n",
        "    print(\"\\nBot could not be initialized due to errors in previous steps. Please check the logs above.\")\n",
        "\n",
        "#@title 7. Simple Command-Line Interface (CLI)\n",
        "\n",
        "if jupiter_bot:\n",
        "    print(\"\\nType your question, or 'quit' to exit.\")\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() == 'quit':\n",
        "            print(\"Exiting chat. Goodbye!\")\n",
        "            break\n",
        "\n",
        "        response = jupiter_bot.ask(user_input)\n",
        "        print(f\"Bot: {response}\")\n",
        "else:\n",
        "    print(\"\\nCannot start chat. Bot was not initialized successfully.\")"
      ]
    }
  ]
}